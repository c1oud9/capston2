"""
ComPile ë°ì´í„°ì…‹ ê¸°ë°˜ IR/Assembly ë°ì´í„°ì…‹ ìƒì„± íŒŒì´í”„ë¼ì¸ (ìµœì í™” ë²„ì „)
ëª©í‘œ: 2-3B í† í° (StarCoder í† í¬ë‚˜ì´ì € ê¸°ì¤€)

ìµœì í™” ì‚¬í•­:
- ëŠë¦° ìŠ¤íŠ¸ë¦¬ë° ëŒ€ì‹  ì‘ì€ ë¡œì»¬ ìƒ˜í”Œ ë‹¤ìš´ë¡œë“œ
- C/C++ í•„í„°ë§í•´ì„œ ì²˜ìŒë¶€í„° ë¶ˆí•„ìš”í•œ ë°ì´í„° ìŠ¤í‚µ ì•ˆ í•¨
- ëª…ì‹œì  ìƒ˜í”Œë§ìœ¼ë¡œ ì •í™•í•˜ê²Œ 0.01% ë‹¤ìš´ë¡œë“œ
"""

import os
import json
import subprocess
import tempfile
from pathlib import Path
from tqdm import tqdm
from datasets import load_dataset
from transformers import AutoTokenizer

# ===== ì„¤ì • =====
TARGET_TOKENS = 10_000_000  # 0.01B (10M) í† í° ëª©í‘œ - í…ŒìŠ¤íŠ¸ìš©
OUTPUT_DIR = Path("./ir_asm_dataset")
CHECKPOINT_FILE = "checkpoint.json"

# ì‚¬ìš©í•  ì–¸ì–´ (C/C++ë§Œ)
LANGUAGES = ['C', 'C++']

# Clang ê²½ë¡œ
CLANG_PATH = "clang"  # Windows: ì „ì²´ ê²½ë¡œ ì§€ì • í•„ìš”í•  ìˆ˜ ìˆìŒ

# ===== ë¡œì»¬ ìºì‹± ì„¤ì • (ì¤‘ìš”!) =====
# ìŠ¤íŠ¸ë¦¬ë° ëŒ€ì‹  ì‘ì€ ìƒ˜í”Œì„ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¹ ë¥´ê²Œ ì²˜ë¦¬
USE_LOCAL_CACHE = True  # Trueë©´ ë¡œì»¬ ë‹¤ìš´ë¡œë“œ, Falseë©´ ìŠ¤íŠ¸ë¦¬ë°
DATASET_PERCENT = 0.01  # ì „ì²´ì˜ 0.01% ë‹¤ìš´ë¡œë“œ (ì•½ 60MB)
                         # C/C++ë§Œ í•„í„°ë§í•˜ë©´ í›¨ì”¬ ë” ì‘ìŒ


class ComPileToIRASMPipeline:
    def __init__(self):
        self.output_dir = OUTPUT_DIR
        self.ir_dir = self.output_dir / "llvm_ir"
        self.asm_dir = self.output_dir / "assembly"
        self.metadata_file = self.output_dir / "metadata.jsonl"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.ir_dir.mkdir(parents=True, exist_ok=True)
        self.asm_dir.mkdir(parents=True, exist_ok=True)
        
        # StarCoder í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ğŸ“¦ StarCoder í† í¬ë‚˜ì´ì € ë¡œë”©...")
        self.tokenizer = AutoTokenizer.from_pretrained("bigcode/starcoderbase-1b")
        
        # í†µê³„
        self.stats = {
            "total_tokens": 0,
            "ir_tokens": 0,
            "asm_tokens": 0,
            "processed_files": 0,
            "success_files": 0,
            "failed_files": 0,
            "failed_reasons": {}
        }
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        self.load_checkpoint()
    
    def load_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if Path(CHECKPOINT_FILE).exists():
            with open(CHECKPOINT_FILE, 'r') as f:
                checkpoint = json.load(f)
                self.stats = checkpoint.get("stats", self.stats)
                print(f"âœ“ ì²´í¬í¬ì¸íŠ¸ ë³µì›: {self.stats['processed_files']}ê°œ íŒŒì¼ ì²˜ë¦¬ë¨")
                print(f"  - í˜„ì¬ í† í°: {self.stats['total_tokens']:,}")
    
    def save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        with open(CHECKPOINT_FILE, 'w') as f:
            json.dump({"stats": self.stats}, f, indent=2)
    
    def bitcode_to_textual_ir(self, bitcode):
        """Bitcodeë¥¼ í…ìŠ¤íŠ¸ IRë¡œ ë³€í™˜"""
        try:
            dis_command = ['llvm-dis', '-']
            with subprocess.Popen(
                dis_command,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                stdin=subprocess.PIPE
            ) as dis_process:
                output, error = dis_process.communicate(input=bitcode, timeout=10)
                
                if dis_process.returncode == 0:
                    return output.decode('utf-8', errors='ignore')
                else:
                    return None
        except Exception as e:
            return None
    
    def ir_to_assembly(self, ir_text):
        """í…ìŠ¤íŠ¸ IRì„ Assemblyë¡œ ë³€í™˜"""
        try:
            # ì„ì‹œ íŒŒì¼ì— IR ì €ì¥
            with tempfile.NamedTemporaryFile(mode='w', suffix='.ll', 
                                            delete=False, encoding='utf-8') as f:
                f.write(ir_text)
                ir_file = f.name
            
            asm_file = ir_file.replace('.ll', '.s')
            
            # clang -S input.ll -o output.s
            result = subprocess.run(
                [CLANG_PATH, '-S', ir_file, '-o', asm_file],
                capture_output=True,
                timeout=10,
                text=True
            )
            
            if result.returncode == 0 and Path(asm_file).exists():
                with open(asm_file, 'r', encoding='utf-8') as f:
                    asm_content = f.read()
                
                # ì •ë¦¬
                os.unlink(ir_file)
                os.unlink(asm_file)
                return asm_content
            else:
                if Path(ir_file).exists():
                    os.unlink(ir_file)
                if Path(asm_file).exists():
                    os.unlink(asm_file)
                return None
                
        except Exception as e:
            return None
    
    def count_tokens(self, text):
        """StarCoder í† í¬ë‚˜ì´ì €ë¡œ í† í° ìˆ˜ ê³„ì‚°"""
        try:
            tokens = self.tokenizer.encode(text, add_special_tokens=False)
            return len(tokens)
        except:
            # í´ë°±: ë‹¨ìˆœ ì¶”ì •
            return len(text.split())
    
    def process_single_file(self, row):
        """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬"""
        try:
            # 1. Bitcode â†’ Textual IR
            bitcode = row['content']
            ir_text = self.bitcode_to_textual_ir(bitcode)
            
            if not ir_text or len(ir_text) < 50:
                self.stats['failed_reasons']['bitcode_conversion'] = \
                    self.stats['failed_reasons'].get('bitcode_conversion', 0) + 1
                return False
            
            # 2. IR â†’ Assembly
            asm_text = self.ir_to_assembly(ir_text)
            
            if not asm_text or len(asm_text) < 50:
                self.stats['failed_reasons']['ir_to_asm'] = \
                    self.stats['failed_reasons'].get('ir_to_asm', 0) + 1
                return False
            
            # 3. íŒŒì¼ ì €ì¥
            file_id = f"{self.stats['success_files']:08d}"
            
            ir_path = self.ir_dir / f"{file_id}.ll"
            with open(ir_path, 'w', encoding='utf-8') as f:
                f.write(ir_text)
            
            asm_path = self.asm_dir / f"{file_id}.s"
            with open(asm_path, 'w', encoding='utf-8') as f:
                f.write(asm_text)
            
            # 4. í† í° ìˆ˜ ê³„ì‚°
            ir_tokens = self.count_tokens(ir_text)
            asm_tokens = self.count_tokens(asm_text)
            
            # 5. ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "file_id": file_id,
                "language": row.get('language', 'unknown'),
                "ir_tokens": ir_tokens,
                "asm_tokens": asm_tokens,
                "total_tokens": ir_tokens + asm_tokens,
                "ir_path": str(ir_path),
                "asm_path": str(asm_path),
                "license": row.get('license_expression', 'unknown')
            }
            
            with open(self.metadata_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(metadata) + '\n')
            
            # 6. í†µê³„ ì—…ë°ì´íŠ¸
            self.stats['ir_tokens'] += ir_tokens
            self.stats['asm_tokens'] += asm_tokens
            self.stats['total_tokens'] = self.stats['ir_tokens'] + self.stats['asm_tokens']
            self.stats['success_files'] += 1
            
            return True
            
        except Exception as e:
            self.stats['failed_reasons']['exception'] = \
                self.stats['failed_reasons'].get('exception', 0) + 1
            return False
    
    def run(self):
        """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("=" * 70)
        print("ComPile ê¸°ë°˜ IR/Assembly ë°ì´í„°ì…‹ ìƒì„± (ìµœì í™” ë²„ì „)")
        print(f"ëª©í‘œ: {TARGET_TOKENS:,} í† í° (StarCoder í† í¬ë‚˜ì´ì €)")
        print("=" * 70)
        
        # Clang ì²´í¬
        try:
            result = subprocess.run([CLANG_PATH, '--version'], 
                                  capture_output=True, text=True, timeout=5)
            clang_version = result.stdout.split('\n')[0]
            print(f"âœ“ Clang ë°œê²¬: {clang_version}")
        except:
            print("âŒ Clangì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print("ì„¤ì¹˜ ë°©ë²•: https://releases.llvm.org/download.html")
            return
        
        # llvm-dis ì²´í¬
        try:
            result = subprocess.run(['llvm-dis', '--version'], 
                                  capture_output=True, text=True, timeout=5)
            print(f"âœ“ llvm-dis ë°œê²¬")
        except:
            print("âŒ llvm-disë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print("llvm-disëŠ” LLVM ì„¤ì¹˜ ì‹œ í¬í•¨ë©ë‹ˆë‹¤.")
            return
        
        # ComPile ë°ì´í„°ì…‹ ë¡œë“œ
        print("\nğŸ“¥ ComPile ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
        
        if USE_LOCAL_CACHE:
            # ì˜µì…˜ 1: ë¡œì»¬ì— ì‘ì€ ìƒ˜í”Œ ë‹¤ìš´ë¡œë“œ (ì¶”ì²œ)
            print(f"  - ë°©ì‹: ë¡œì»¬ ë‹¤ìš´ë¡œë“œ ({DATASET_PERCENT*100}%)")
            print(f"  - ì•½ {600 * DATASET_PERCENT*100:.0f}MB ë‹¤ìš´ë¡œë“œ ì˜ˆìƒ")
            print("  â³ ì²« ë¡œë“œëŠ” ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë‹¤ìŒë¶€í„°ëŠ” ìºì‹œ ì‚¬ìš©)...\n")
            
            # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
            print("  â†’ ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
            ds = load_dataset('llvm-ml/ComPile', split='train')
            print(f"  âœ“ ë¡œë“œ ì™„ë£Œ: {len(ds):,}ê°œ íŒŒì¼")
            
            # C/C++ë§Œ í•„í„°ë§
            print(f"ğŸ” í•„í„°ë§: {', '.join(LANGUAGES)}")
            ds = ds.filter(lambda x: x['language'] in LANGUAGES)
            print(f"âœ“ í•„í„°ë§ ì™„ë£Œ ({len(ds):,}ê°œ íŒŒì¼)\n")
            
            # ëª…ì‹œì ìœ¼ë¡œ ìƒ˜í”Œ ì„ íƒ
            print(f"ğŸ“Š {DATASET_PERCENT*100}% ìƒ˜í”Œë§ ì¤‘...")
            sample_size = max(1, int(len(ds) * DATASET_PERCENT))
            ds = ds.select(range(sample_size))
            print(f"âœ“ {sample_size:,}ê°œ íŒŒì¼ ì„ íƒë¨\n")
        else:
            # ì˜µì…˜ 2: ìŠ¤íŠ¸ë¦¬ë° (ëŠë¦¼, ë¹„ì¶”ì²œ)
            print(f"  - ë°©ì‹: ìŠ¤íŠ¸ë¦¬ë°")
            print("  âš ï¸  ì´ ë°©ì‹ì€ ëŠë¦¬ë‹ˆ USE_LOCAL_CACHE=True ì¶”ì²œ\n")
            
            ds = load_dataset('llvm-ml/ComPile', split='train', streaming=True)
            
            # C/C++ë§Œ í•„í„°ë§
            print(f"ğŸ” í•„í„°ë§: {', '.join(LANGUAGES)}")
            ds = ds.filter(lambda x: x['language'] in LANGUAGES)
            print(f"âœ“ í•„í„°ë§ ì™„ë£Œ\n")
        
        # ì§„í–‰ë¥  ë°”
        pbar = tqdm(total=TARGET_TOKENS, unit='tokens', 
                   desc="ì§„í–‰", unit_scale=True)
        pbar.update(self.stats['total_tokens'])
        
        # ì²˜ë¦¬ ì‹œì‘
        print("ğŸš€ ì²˜ë¦¬ ì‹œì‘...\n")
        
        for row in ds:
            if self.stats['total_tokens'] >= TARGET_TOKENS:
                print("\nâœ… ëª©í‘œ í† í° ë„ë‹¬!")
                break
            
            self.stats['processed_files'] += 1
            
            success = self.process_single_file(row)
            
            if success:
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                token_increase = self.stats['total_tokens'] - pbar.n
                pbar.update(token_increase)
                pbar.set_postfix({
                    'Success': self.stats['success_files'],
                    'Fail': self.stats['failed_files'],
                    'Rate': f"{self.stats['success_files']*100/(self.stats['processed_files'] or 1):.1f}%"
                })
                # ì½˜ì†”ì—ë„ ì¶œë ¥ (ë°±ê·¸ë¼ìš´ë“œìš©)
                if self.stats['success_files'] % 10 == 0:
                    print(f"\n[{self.stats['success_files']}] í† í°: {self.stats['total_tokens']:,} | ì„±ê³µë¥ : {self.stats['success_files']*100/(self.stats['processed_files'] or 1):.1f}%")
            else:
                self.stats['failed_files'] += 1
            
            # 100ê°œë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if self.stats['processed_files'] % 100 == 0:
                self.save_checkpoint()
        
        pbar.close()
        
        # ìµœì¢… í†µê³„
        self.print_final_stats()
        self.save_checkpoint()
    
    def print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        print("\n" + "=" * 70)
        print("ì™„ë£Œ!")
        print("=" * 70)
        print(f"ì²˜ë¦¬ëœ íŒŒì¼:      {self.stats['processed_files']:,}")
        print(f"ì„±ê³µ:            {self.stats['success_files']:,}")
        print(f"ì‹¤íŒ¨:            {self.stats['failed_files']:,}")
        print(f"ì„±ê³µë¥ :          {self.stats['success_files']*100/(self.stats['processed_files'] or 1):.1f}%")
        
        print(f"\ní† í° ìˆ˜ (StarCoder í† í¬ë‚˜ì´ì €):")
        print(f"  IR í† í°:       {self.stats['ir_tokens']:,}")
        print(f"  ASM í† í°:      {self.stats['asm_tokens']:,}")
        print(f"  ì´ í† í°:       {self.stats['total_tokens']:,}")
        
        print(f"\nì‹¤íŒ¨ ì›ì¸:")
        for reason, count in self.stats['failed_reasons'].items():
            print(f"  {reason}: {count:,}")
        
        print(f"\nì¶œë ¥ ë””ë ‰í† ë¦¬:   {self.output_dir}")
        print(f"  - IR:          {self.ir_dir}")
        print(f"  - Assembly:    {self.asm_dir}")
        print(f"  - Metadata:    {self.metadata_file}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    pipeline = ComPileToIRASMPipeline()
    pipeline.run()


if __name__ == "__main__":
    main()